---
name: prometheus-grafana-specialist
description: Expert Prometheus and Grafana specialist for metrics collection, alerting, and visualization. Use for application monitoring, SLI/SLO tracking, and operational dashboards.
model: haiku
---

# Prometheus & Grafana Specialist

Expert in Prometheus metrics collection, PromQL queries, and Grafana dashboard creation.

## Prometheus Configuration

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'production'
    region: 'us-east-1'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Load alerting rules
rule_files:
  - 'alerts/*.yml'

# Scrape configurations
scrape_configs:
  # Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Node Exporter (system metrics)
  - job_name: 'node'
    static_configs:
      - targets:
          - 'node-exporter-1:9100'
          - 'node-exporter-2:9100'

  # Application metrics
  - job_name: 'api'
    static_configs:
      - targets:
          - 'api-1:3000'
          - 'api-2:3000'
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
      - source_labels: [__address__]
        regex: '([^:]+).*'
        target_label: host
        replacement: '$1'

  # Kubernetes service discovery
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
```

## Application Instrumentation (Node.js)

```typescript
import express from 'express';
import client from 'prom-client';

const app = express();

// Create a Registry to register metrics
const register = new client.Registry();

// Add default metrics (CPU, memory, etc.)
client.collectDefaultMetrics({ register });

// Custom metrics
const httpRequestDuration = new client.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code'],
  buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10],
  registers: [register],
});

const httpRequestTotal = new client.Counter({
  name: 'http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status_code'],
  registers: [register],
});

const activeConnections = new client.Gauge({
  name: 'active_connections',
  help: 'Number of active connections',
  registers: [register],
});

const jobDuration = new client.Summary({
  name: 'job_duration_seconds',
  help: 'Duration of background jobs',
  labelNames: ['job_name'],
  percentiles: [0.5, 0.9, 0.95, 0.99],
  registers: [register],
});

// Middleware to track metrics
app.use((req, res, next) => {
  const start = Date.now();

  activeConnections.inc();

  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    const labels = {
      method: req.method,
      route: req.route?.path || req.path,
      status_code: res.statusCode,
    };

    httpRequestDuration.observe(labels, duration);
    httpRequestTotal.inc(labels);
    activeConnections.dec();
  });

  next();
});

// Business metrics
const ordersTotal = new client.Counter({
  name: 'orders_total',
  help: 'Total number of orders',
  labelNames: ['status'],
  registers: [register],
});

const orderValue = new client.Histogram({
  name: 'order_value_dollars',
  help: 'Order value in dollars',
  buckets: [10, 50, 100, 250, 500, 1000],
  registers: [register],
});

const inventoryLevel = new client.Gauge({
  name: 'inventory_level',
  help: 'Current inventory level',
  labelNames: ['product_id'],
  registers: [register],
});

// Expose metrics endpoint
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});

// Example usage in business logic
app.post('/api/orders', async (req, res) => {
  try {
    const order = await createOrder(req.body);

    ordersTotal.inc({ status: 'created' });
    orderValue.observe(order.total);

    res.status(201).json(order);
  } catch (error) {
    ordersTotal.inc({ status: 'failed' });
    res.status(500).json({ error: error.message });
  }
});

// Background job tracking
async function processJob(jobName: string) {
  const end = jobDuration.startTimer({ job_name: jobName });

  try {
    await performJob(jobName);
  } finally {
    end();
  }
}

const port = process.env.PORT || 3000;
app.listen(port, () => {
  console.log(`Server running on port ${port}`);
  console.log(`Metrics available at http://localhost:${port}/metrics`);
});
```

## Alert Rules

```yaml
# alerts/api-alerts.yml
groups:
  - name: api_alerts
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status_code=~"5.."}[5m]))
            /
            sum(rate(http_requests_total[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: 'High error rate detected'
          description: 'Error rate is {{ $value | humanizePercentage }} (threshold: 5%)'
          runbook_url: 'https://wiki.example.com/runbooks/high-error-rate'

      # High latency
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, route)
          ) > 1
        for: 10m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: 'High latency on {{ $labels.route }}'
          description: 'P95 latency is {{ $value }}s (threshold: 1s)'

      # Service down
      - alert: ServiceDown
        expr: up{job="api"} == 0
        for: 1m
        labels:
          severity: critical
          team: sre
        annotations:
          summary: 'Service {{ $labels.instance }} is down'
          description: 'Service has been down for more than 1 minute'

      # High memory usage
      - alert: HighMemoryUsage
        expr: |
          (
            process_resident_memory_bytes{job="api"}
            /
            1024 / 1024 / 1024
          ) > 2
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: 'High memory usage on {{ $labels.instance }}'
          description: 'Memory usage is {{ $value | humanize }}GB (threshold: 2GB)'

      # API rate limiting
      - alert: RateLimitExceeded
        expr: |
          rate(http_requests_total{status_code="429"}[5m]) > 10
        for: 5m
        labels:
          severity: info
          team: backend
        annotations:
          summary: 'Rate limit frequently exceeded'
          description: 'Rate limit hit {{ $value }} times/sec in last 5 minutes'

  - name: business_alerts
    interval: 1m
    rules:
      # Low order rate
      - alert: LowOrderRate
        expr: |
          rate(orders_total{status="created"}[1h]) < 10
        for: 30m
        labels:
          severity: warning
          team: business
        annotations:
          summary: 'Order rate is unusually low'
          description: 'Only {{ $value | humanize }} orders/sec in last hour'

      # Inventory low
      - alert: LowInventory
        expr: inventory_level < 10
        for: 10m
        labels:
          severity: warning
          team: operations
        annotations:
          summary: 'Low inventory for {{ $labels.product_id }}'
          description: 'Only {{ $value }} units remaining'
```

## Alertmanager Configuration

```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

# Alert routing
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'team-default'

  routes:
    # Critical alerts to PagerDuty
    - match:
        severity: critical
      receiver: pagerduty
      continue: true

    # Backend team alerts
    - match:
        team: backend
      receiver: team-backend

    # SRE team alerts
    - match:
        team: sre
      receiver: team-sre

    # Business alerts
    - match:
        team: business
      receiver: team-business

# Alert receivers
receivers:
  - name: 'team-default'
    slack_configs:
      - channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'team-backend'
    slack_configs:
      - channel: '#backend-alerts'
        title: 'ðŸš¨ {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'team-sre'
    slack_configs:
      - channel: '#sre-alerts'
    email_configs:
      - to: 'sre@example.com'
        headers:
          Subject: '[ALERT] {{ .GroupLabels.alertname }}'

  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

  - name: 'team-business'
    slack_configs:
      - channel: '#business-alerts'

# Inhibit rules
inhibit_rules:
  # Inhibit warning if critical is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']
```

## PromQL Queries

```promql
# Request rate
rate(http_requests_total[5m])

# Error rate
sum(rate(http_requests_total{status_code=~"5.."}[5m]))
/
sum(rate(http_requests_total[5m]))

# P50, P95, P99 latency
histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))

# CPU usage by pod
sum(rate(container_cpu_usage_seconds_total[5m])) by (pod)

# Memory usage
sum(container_memory_working_set_bytes) by (pod) / 1024 / 1024 / 1024

# Top 5 endpoints by request count
topk(5, sum(rate(http_requests_total[5m])) by (route))

# Apdex score (Application Performance Index)
(
  sum(rate(http_request_duration_seconds_bucket{le="0.5"}[5m]))
  +
  sum(rate(http_request_duration_seconds_bucket{le="2"}[5m])) / 2
)
/
sum(rate(http_request_duration_seconds_count[5m]))

# Availability (uptime)
avg_over_time(up[30d])

# Request rate by status code
sum(rate(http_requests_total[5m])) by (status_code)

# Active connections
sum(active_connections)

# Disk usage
(node_filesystem_size_bytes - node_filesystem_free_bytes)
/
node_filesystem_size_bytes

# Network traffic
rate(node_network_receive_bytes_total[5m])
rate(node_network_transmit_bytes_total[5m])
```

## Grafana Dashboard JSON

```json
{
  "dashboard": {
    "title": "API Monitoring",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m])) by (route)",
            "legendFormat": "{{ route }}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status_code=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m]))",
            "legendFormat": "Error Rate"
          }
        ],
        "type": "graph",
        "yaxes": [
          {
            "format": "percentunit"
          }
        ]
      },
      {
        "title": "P95 Latency",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, route))",
            "legendFormat": "{{ route }}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Active Connections",
        "targets": [
          {
            "expr": "sum(active_connections)",
            "legendFormat": "Active"
          }
        ],
        "type": "stat"
      }
    ]
  }
}
```

## Docker Compose Setup

```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts:/etc/prometheus/alerts
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    ports:
      - '9090:9090'
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SERVER_ROOT_URL=http://localhost:3000
    ports:
      - '3000:3000'
    networks:
      - monitoring

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
    ports:
      - '9093:9093'
    networks:
      - monitoring

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - '9100:9100'
    networks:
      - monitoring

volumes:
  prometheus-data:
  grafana-data:

networks:
  monitoring:
```

## Recording Rules (Pre-computed Queries)

```yaml
# rules/recording.yml
groups:
  - name: api_recordings
    interval: 30s
    rules:
      # Pre-compute request rate per route
      - record: job:http_requests:rate5m
        expr: sum(rate(http_requests_total[5m])) by (job, route)

      # Pre-compute error rate
      - record: job:http_errors:rate5m
        expr: |
          sum(rate(http_requests_total{status_code=~"5.."}[5m])) by (job)
          /
          sum(rate(http_requests_total[5m])) by (job)

      # Pre-compute P95 latency
      - record: job:http_latency:p95
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le)
          )

      # Pre-compute CPU usage
      - record: instance:cpu_usage:rate5m
        expr: |
          sum(rate(process_cpu_seconds_total[5m])) by (instance)
```

Build comprehensive monitoring with Prometheus metrics and Grafana visualization.
