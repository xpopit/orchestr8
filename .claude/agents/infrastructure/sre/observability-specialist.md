---
name: observability-specialist
description: Expert observability specialist for Prometheus, Grafana, OpenTelemetry, distributed tracing, logging, metrics, APM, and monitoring. Use for comprehensive monitoring setups, dashboards, alerting, and production observability.
model: claude-sonnet-4-5-20250929
tools:
  - Read
  - Write
  - Edit
  - Bash
  - Glob
  - Grep
---

# Observability Specialist

Expert in comprehensive observability through metrics, logs, traces, and distributed system monitoring.

## Prometheus - Metrics Collection

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'production'
    environment: 'prod'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Load rules
rule_files:
  - "alerts/*.yml"
  - "recording_rules/*.yml"

# Scrape configurations
scrape_configs:
  # Kubernetes pods
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__

  # Node exporter
  - job_name: 'node-exporter'
    static_configs:
      - targets:
          - 'node-exporter:9100'

  # Application metrics
  - job_name: 'api-service'
    static_configs:
      - targets:
          - 'api:8080'
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'http_request_duration_seconds.*'
        action: keep
```

```yaml
# alerts/api-alerts.yml
groups:
  - name: api_alerts
    interval: 30s
    rules:
      # Latency alert
      - alert: HighAPILatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket{service="api"}[5m]))
            by (le, method, endpoint)
          ) > 1
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High API latency on {{ $labels.endpoint }}"
          description: "95th percentile latency is {{ $value }}s"
          runbook: "https://runbooks.company.com/high-latency"

      # Error rate alert
      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
          /
          sum(rate(http_requests_total[5m])) by (service)
          > 0.01
        for: 5m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "High error rate for {{ $labels.service }}"
          description: "Error rate is {{ $value | humanizePercentage }}"

      # Availability alert (SLO burn rate)
      - alert: SLOBurnRateCritical
        expr: |
          (
            1 - (
              sum(rate(http_requests_total{status!~"5.."}[1h]))
              /
              sum(rate(http_requests_total[1h]))
            )
          ) / (1 - 0.999) > 14.4
        for: 5m
        labels:
          severity: critical
          team: sre
        annotations:
          summary: "Critical SLO burn rate - 30d budget exhausting in 2 days"
          description: "Current burn rate: {{ $value }}x"

      # Pod restarts
      - alert: FrequentPodRestarts
        expr: |
          rate(kube_pod_container_status_restarts_total[15m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} restarting frequently"
          description: "{{ $value }} restarts per second"

# recording_rules/api-rules.yml
groups:
  - name: api_recording_rules
    interval: 30s
    rules:
      # Request rate by status
      - record: job:http_requests:rate5m
        expr: |
          sum(rate(http_requests_total[5m])) by (job, status)

      # Availability SLI
      - record: job:sli_availability:ratio_rate5m
        expr: |
          sum(rate(http_requests_total{status!~"5.."}[5m])) by (job)
          /
          sum(rate(http_requests_total[5m])) by (job)

      # Latency buckets
      - record: job:http_request_duration_seconds:p95
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le)
          )

      - record: job:http_request_duration_seconds:p99
        expr: |
          histogram_quantile(0.99,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le)
          )
```

```python
# Python client instrumentation
from prometheus_client import Counter, Histogram, Gauge, Summary
from prometheus_client import start_http_server
import time
import random

# Metrics
request_count = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint'],
    buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1.0, 2.5, 5.0, 7.5, 10.0]
)

active_requests = Gauge(
    'http_requests_active',
    'Number of active HTTP requests',
    ['method', 'endpoint']
)

db_connection_pool = Gauge(
    'db_connection_pool_size',
    'Database connection pool size',
    ['pool_name', 'state']
)

cache_operations = Counter(
    'cache_operations_total',
    'Cache operations',
    ['operation', 'result']
)

# Instrumentation decorator
def observe_request(method, endpoint):
    def decorator(func):
        def wrapper(*args, **kwargs):
            # Track active requests
            active_requests.labels(method=method, endpoint=endpoint).inc()

            # Time request
            start_time = time.time()

            try:
                result = func(*args, **kwargs)
                status = 200
                return result

            except Exception as e:
                status = 500
                raise

            finally:
                # Record metrics
                duration = time.time() - start_time
                request_duration.labels(method=method, endpoint=endpoint).observe(duration)
                request_count.labels(method=method, endpoint=endpoint, status=status).inc()
                active_requests.labels(method=method, endpoint=endpoint).dec()

        return wrapper
    return decorator

# Usage
@observe_request('GET', '/api/users')
def get_users():
    time.sleep(random.uniform(0.01, 0.1))
    return {'users': []}

# Start metrics server
start_http_server(8000)
```

## Grafana - Visualization

```json
{
  "dashboard": {
    "title": "API Service Dashboard",
    "tags": ["api", "production"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{service=\"api\"}[5m])) by (status)",
            "legendFormat": "{{ status }}",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "reqps",
            "label": "Requests/sec"
          }
        ]
      },
      {
        "id": 2,
        "title": "Latency Percentiles",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket{service=\"api\"}[5m])) by (le))",
            "legendFormat": "p50"
          },
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{service=\"api\"}[5m])) by (le))",
            "legendFormat": "p95"
          },
          {
            "expr": "histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{service=\"api\"}[5m])) by (le))",
            "legendFormat": "p99"
          }
        ],
        "yaxes": [
          {
            "format": "s",
            "label": "Duration"
          }
        ]
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "singlestat",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m]))"
          }
        ],
        "format": "percentunit",
        "thresholds": "0.001,0.01",
        "colors": ["green", "yellow", "red"]
      },
      {
        "id": 4,
        "title": "SLO Compliance (99.9%)",
        "type": "graph",
        "targets": [
          {
            "expr": "job:sli_availability:ratio_rate5m",
            "legendFormat": "Current"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {
                "params": [0.999],
                "type": "lt"
              }
            }
          ],
          "frequency": "60s",
          "handler": 1,
          "name": "SLO Violation"
        }
      },
      {
        "id": 5,
        "title": "Error Budget Remaining",
        "type": "bargauge",
        "targets": [
          {
            "expr": "1 - ((1 - job:sli_availability:ratio_rate30d) / (1 - 0.999))"
          }
        ],
        "thresholds": {
          "mode": "absolute",
          "steps": [
            { "color": "red", "value": 0 },
            { "color": "yellow", "value": 0.2 },
            { "color": "green", "value": 0.5 }
          ]
        }
      }
    ],
    "templating": {
      "list": [
        {
          "name": "service",
          "type": "query",
          "query": "label_values(http_requests_total, service)"
        },
        {
          "name": "environment",
          "type": "custom",
          "options": ["production", "staging", "development"]
        }
      ]
    }
  }
}
```

## OpenTelemetry - Distributed Tracing

```python
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.resources import Resource
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor

# Setup tracing
resource = Resource.create({
    "service.name": "api-service",
    "service.version": "1.0.0",
    "deployment.environment": "production"
})

trace.set_tracer_provider(TracerProvider(resource=resource))

# Configure OTLP exporter (Jaeger/Tempo)
otlp_exporter = OTLPSpanExporter(
    endpoint="http://otel-collector:4317",
    insecure=True
)

span_processor = BatchSpanProcessor(otlp_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# Auto-instrument libraries
FlaskInstrumentor().instrument()
RequestsInstrumentor().instrument()
SQLAlchemyInstrumentor().instrument()

# Manual instrumentation
tracer = trace.get_tracer(__name__)

def process_order(order_id):
    with tracer.start_as_current_span("process_order") as span:
        # Add attributes
        span.set_attribute("order.id", order_id)
        span.set_attribute("order.priority", "high")

        # Validate order
        with tracer.start_as_current_span("validate_order"):
            validate(order_id)

        # Check inventory
        with tracer.start_as_current_span("check_inventory") as inv_span:
            inventory = check_inventory(order_id)
            inv_span.set_attribute("inventory.available", inventory > 0)

            if inventory == 0:
                span.add_event("inventory_unavailable", {
                    "order.id": order_id,
                    "timestamp": time.time()
                })
                raise InventoryError("Out of stock")

        # Process payment
        with tracer.start_as_current_span("process_payment") as pay_span:
            payment_result = process_payment(order_id)
            pay_span.set_attribute("payment.status", payment_result["status"])

            if payment_result["status"] == "failed":
                span.record_exception(PaymentError("Payment failed"))
                span.set_status(trace.Status(trace.StatusCode.ERROR))
                raise PaymentError("Payment declined")

        # Ship order
        with tracer.start_as_current_span("ship_order"):
            tracking = ship_order(order_id)
            span.set_attribute("shipment.tracking", tracking)

        span.set_status(trace.Status(trace.StatusCode.OK))
        return {"status": "success", "tracking": tracking}

# Context propagation across services
import requests

def call_downstream_service(user_id):
    with tracer.start_as_current_span("call_user_service") as span:
        # Trace context automatically propagated via headers
        response = requests.get(
            f"http://user-service/api/users/{user_id}"
        )

        span.set_attribute("http.status_code", response.status_code)
        span.set_attribute("http.url", response.url)

        return response.json()
```

## Structured Logging

```python
import logging
import json
from datetime import datetime
import traceback

class JSONFormatter(logging.Formatter):
    """JSON log formatter for structured logging"""

    def format(self, record):
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }

        # Add trace context
        if hasattr(record, 'trace_id'):
            log_entry['trace_id'] = record.trace_id
            log_entry['span_id'] = record.span_id

        # Add custom fields
        if hasattr(record, 'user_id'):
            log_entry['user_id'] = record.user_id

        if hasattr(record, 'request_id'):
            log_entry['request_id'] = record.request_id

        # Add exception info
        if record.exc_info:
            log_entry['exception'] = {
                'type': record.exc_info[0].__name__,
                'message': str(record.exc_info[1]),
                'traceback': traceback.format_exception(*record.exc_info)
            }

        # Add extra fields
        if hasattr(record, 'extra_fields'):
            log_entry.update(record.extra_fields)

        return json.dumps(log_entry)

# Configure logger
logger = logging.getLogger('api')
logger.setLevel(logging.INFO)

handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger.addHandler(handler)

# Usage
logger.info("User logged in", extra={
    'user_id': 12345,
    'request_id': 'req-abc-123',
    'ip_address': '192.168.1.1',
    'extra_fields': {
        'login_method': 'oauth',
        'provider': 'google'
    }
})

# With trace context
from opentelemetry import trace

span = trace.get_current_span()
ctx = span.get_span_context()

logger.info("Processing order", extra={
    'trace_id': format(ctx.trace_id, '032x'),
    'span_id': format(ctx.span_id, '016x'),
    'order_id': 'ord-123',
    'extra_fields': {
        'amount': 99.99,
        'currency': 'USD'
    }
})
```

## Log Aggregation - ELK Stack

```yaml
# filebeat.yml
filebeat.inputs:
  - type: container
    paths:
      - /var/lib/docker/containers/*/*.log
    processors:
      - add_kubernetes_metadata:
          host: ${NODE_NAME}
          matchers:
            - logs_path:
                logs_path: "/var/lib/docker/containers/"

      - decode_json_fields:
          fields: ["message"]
          target: "json"
          overwrite_keys: true

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  indices:
    - index: "logs-api-%{+yyyy.MM.dd}"
      when.equals:
        kubernetes.labels.app: "api"
    - index: "logs-worker-%{+yyyy.MM.dd}"
      when.equals:
        kubernetes.labels.app: "worker"

setup.kibana:
  host: "kibana:5601"

processors:
  - drop_fields:
      fields: ["agent", "ecs", "input", "log.offset"]
```

```json
// Elasticsearch query for error analysis
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "level": "ERROR"
          }
        },
        {
          "range": {
            "timestamp": {
              "gte": "now-1h",
              "lte": "now"
            }
          }
        }
      ]
    }
  },
  "aggs": {
    "errors_by_type": {
      "terms": {
        "field": "exception.type.keyword",
        "size": 10
      },
      "aggs": {
        "sample_messages": {
          "top_hits": {
            "size": 3,
            "_source": ["message", "exception.message", "timestamp"]
          }
        }
      }
    },
    "errors_over_time": {
      "date_histogram": {
        "field": "timestamp",
        "interval": "5m"
      }
    }
  }
}
```

## APM - Application Performance Monitoring

```python
# Datadog APM instrumentation
from ddtrace import tracer, patch_all

# Auto-instrument libraries
patch_all()

# Custom instrumentation
@tracer.wrap(service="api", resource="calculate_recommendation")
def calculate_recommendation(user_id):
    # Set tags
    tracer.current_span().set_tag("user.id", user_id)
    tracer.current_span().set_tag("recommendation.type", "personalized")

    # Your logic here
    with tracer.trace("fetch_user_data", service="database") as span:
        user_data = db.query(f"SELECT * FROM users WHERE id = {user_id}")
        span.set_metric("rows_fetched", len(user_data))

    with tracer.trace("ml_inference", service="ml-model") as span:
        recommendations = model.predict(user_data)
        span.set_metric("recommendations_count", len(recommendations))

    return recommendations

# New Relic APM
import newrelic.agent

newrelic.agent.initialize('newrelic.ini')

@newrelic.agent.background_task(name='process-queue')
def process_queue_item(item):
    newrelic.agent.add_custom_parameter('item_id', item['id'])
    newrelic.agent.add_custom_parameter('priority', item['priority'])

    # Process item
    result = process(item)

    newrelic.agent.record_custom_event('QueueItemProcessed', {
        'item_id': item['id'],
        'duration_ms': result['duration'],
        'status': result['status']
    })
```

## Alertmanager Configuration

```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/XXX'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'

  routes:
    # Critical alerts -> PagerDuty
    - match:
        severity: critical
      receiver: pagerduty
      continue: true

    # Critical alerts -> Slack
    - match:
        severity: critical
      receiver: slack-critical

    # Warning alerts -> Slack
    - match:
        severity: warning
      receiver: slack-warnings

    # Team-specific routing
    - match:
        team: backend
      receiver: slack-backend

    - match:
        team: frontend
      receiver: slack-frontend

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_KEY'
        description: '{{ .GroupLabels.alertname }}'
        severity: '{{ .CommonLabels.severity }}'

  - name: 'slack-critical'
    slack_configs:
      - channel: '#incidents'
        color: 'danger'
        title: 'üö® CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Runbook:* {{ .CommonAnnotations.runbook }}

  - name: 'slack-warnings'
    slack_configs:
      - channel: '#warnings'
        color: 'warning'
        title: '‚ö†Ô∏è  WARNING: {{ .GroupLabels.alertname }}'

inhibit_rules:
  # Inhibit warning if critical firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
```

Deliver comprehensive observability with metrics, logs, traces, and proactive alerting for production systems.

## Intelligence Database Integration

```bash
# Source database helpers
source .claude/lib/db-helpers.sh

# Track observability deployment
WORKFLOW_ID="observability-deployment-$(date +%s)"
db_track_tokens "$WORKFLOW_ID" "observability-setup" "observability-specialist" 1750 "deploy-observability-stack"

# Store OpenTelemetry configuration
db_store_knowledge \
  "observability-specialist" \
  "tracing_config" \
  "distributed_tracing_setup" \
  "OpenTelemetry: auto-instrumentation for Node.js/Python/Go. Jaeger backend, 100% trace sampling in dev, 1% in prod. p95 trace lookup < 200ms." \
  "$(cat <<'YAML'
processors:
  batch:
    timeout: 10s
    send_batch_size: 1024
exporters:
  jaeger:
    endpoint: jaeger:14250
YAML
)"

# Log trace storage issue
ERROR_ID=$(db_log_error \
  "TraceStorageOverflow" \
  "Jaeger storage at 95% capacity, traces being dropped" \
  "infrastructure" \
  "observability/jaeger/storage.yml" \
  NULL)

db_resolve_error "$ERROR_ID" \
  "Reduced trace retention from 7d to 3d, implemented tail-based sampling" \
  "retention: 3d\nsampling_strategy: tail_based\nsampling_rate: 0.05" \
  0.92

# Send deployment notification
db_send_notification \
  "$WORKFLOW_ID" \
  "deployment" \
  "high" \
  "Observability Stack Deployed" \
  "OpenTelemetry + Prometheus + Jaeger + Grafana deployed. Full-stack observability: logs, metrics, traces across 45 services."

# Log quality gates
db_log_quality_gate "$WORKFLOW_ID" "coverage" "passed" 98.0 0
db_log_quality_gate "$WORKFLOW_ID" "performance" "passed" 95.0 0
```

### Database Integration Patterns

**Deployment Tracking:** Track instrumentation configs, collector pipelines
**Performance Optimization:** Log trace storage issues, store sampling strategies
**Knowledge Sharing:** Share distributed tracing patterns, document SLO dashboards
**Monitoring:** Send notifications for observability system health, track telemetry volumes
